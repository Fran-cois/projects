{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "  # Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "# Lars Buitinck\n",
    "# Chyi-Kwei Yau <chyikwei.yau@gmail.com> # License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function \n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np \n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "Verbose = DEBUG\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words,Verbose = True): \n",
    "    array_of_message = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1] ])\n",
    "        array_of_message.append(message)\n",
    "        if(Verbose):\n",
    "            print(message) \n",
    "    if(Verbose):\n",
    "        print()\n",
    "    return array_of_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_newspaper(Verbose = False):\n",
    "    # Load the 20 newsgroups dataset and vectorize it. We use a few\n",
    "    # heuristics to filter out useless terms early on: the posts are\n",
    "    # stripped of headers, footers and quoted replies, and common\n",
    "    # English words, words occurring in only one document or in at\n",
    "    # least 95% of the documents are removed.\n",
    "    if(Verbose== True):\n",
    "        print(\"Loading dataset...\")\n",
    "    t0 = time()\n",
    "    dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                           remove=('headers', 'footers', 'quotes'))\n",
    "    data_samples = dataset.data[:n_samples]\n",
    "    if(Verbose):\n",
    "        print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    return dataset, data_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf(Verbose=False):\n",
    "    # Use tf-idf features for NMF.\n",
    "    if(Verbose):\n",
    "        print(\"Extracting tf-idf features...\")\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "    t0 = time()\n",
    "    tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "    if(Verbose== True):\n",
    "        print(\"done in %0.3fs.\" % (time() - t0))\n",
    "        \n",
    "    return tfidf_vectorizer, tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_messages(i):\n",
    "    print(random_messages[i]) # sqrt(X.mean() / n_components)\n",
    "    print(nndsvd_messages[i])  # better sparseness\n",
    "    print(nndsvda_messages[i]) # when sparsity is not desired\n",
    "    print(nndsvdar_messages[i]) # faster, less accurate alternative to NNDSVDa\n",
    "def print_messages_kull(i):\n",
    "    print(random_messages_kull[i]) # sqrt(X.mean() / n_components)\n",
    "    print(nndsvd_messages_kull[i])  # better sparseness\n",
    "    print(nndsvda_messages_kull[i]) # when sparsity is not desired\n",
    "    print(nndsvdar_messages_kull[i]) # faster, less accurate alternative to NNDSVDa\n",
    "def print_messages_it(i):\n",
    "    print(random_messages_it[i]) # sqrt(X.mean() / n_components)\n",
    "    print(nndsvd_messages_it[i])  # better sparseness\n",
    "    print(nndsvda_messages_it[i]) # when sparsity is not desired\n",
    "    print(nndsvdar_messages_it[i]) # faster, less accurate alternative to NNDSVDa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_diff(a,b,verbose_name):\n",
    "    for i in range(len(a)):\n",
    "        if (a[i] != b[i]):\n",
    "            print(\"they are different\" + \" \"+ verbose_name)\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printErrorAndIter(dictOfNmf):\n",
    "    print(\"iterations \\t\",\"error\\t\\t\",\"name\")\n",
    "    for nmf in dictOfNmf.keys():    \n",
    "        print(nmf.n_iter_,\"\\t \\t\",nmf.reconstruction_err_, \" nmf :\", dictOfNmf[nmf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, data_samples = load_data_newspaper()\n",
    "tfidf_vectorizer, tfidf = create_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NMF(init = \"random\",fixed_W = None, random_state = 1, beta_loss='frobenius', solver=\"cd\",\n",
    "              tfidf=tfidf, tfidf_vectorizer=tfidf_vectorizer, Verbose=False):\n",
    "    # Fit the NMF model\n",
    "    if(Verbose):\n",
    "        print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \" \"n_samples=%d and n_features=%d...\" % (n_samples, n_features))\n",
    "    t0 = time()\n",
    "    if(init != \"custom\"):\n",
    "        nmf = NMF(n_components=n_components,init = init,solver=solver, random_state = random_state,beta_loss=beta_loss, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "    else:\n",
    "        fixed_H = fixed_W.T\n",
    "        if(Verbose):\n",
    "            print(fixed_W.shape)\n",
    "            print(fixed_H.shape)\n",
    "        #H : array-like, shape (n_components, n_features)\n",
    "        nmf = NMF(n_components=n_components,init = init,solver=solver, random_state = random_state,beta_loss=beta_loss, alpha=.1, l1_ratio=.5).fit_transform(tfidf,H = fixed_H,W = fixed_W)\n",
    "        \n",
    "    #init : ‘random’ | ‘nndsvd’ | ‘nndsvda’ | ‘nndsvdar’ | ‘custom’\n",
    "    # random_state is the seed used by the random number generator\n",
    "    # alpha is Constant that multiplies the regularization terms\n",
    "    # l1_ratio The regularization mixing parameter between the l1 and l2 norm\n",
    "    if(Verbose):\n",
    "        \n",
    "        print(\"done in %0.3fs.\" % (time() - t0))\n",
    "        print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "        \n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names() \n",
    "    array_of_message = print_top_words(nmf, tfidf_feature_names, n_top_words,Verbose)\n",
    "        \n",
    "    return nmf, array_of_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_random, random_messages = build_NMF(Verbose=Verbose)\n",
    "nmf_nndsvd, nndsvd_messages = build_NMF(init =\"nndsvd\", Verbose=Verbose)\n",
    "nmf_nndsvda, nndsvda_messages = build_NMF(init =\"nndsvda\", Verbose=Verbose)\n",
    "nmf_nndsvdar, nndsvdar_messages = build_NMF(init =\"nndsvdar\", Verbose=Verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(n_samples,n_components)\n",
    "# print(np.all(np.isfinite(W)))\n",
    "# W : array-like, shape (n_samples, n_components)\n",
    "# Custom_messages = build_NMF(init =\"custom\",fixed_W = W, Verbose=False)[1]\n",
    "# there is a problem with the shape of W, H that i did not understand :\n",
    "# I used the shapes told in the documentation of sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Test and comment on the effect of varying the initialisation, especially using random nonnegative values as initial guesses (for W and H coefficients, using the notations in- troduced during the lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations \t error\t\t name\n",
      "104 \t \t 42.183446447525256  nmf : nmf_random\n",
      "110 \t \t 42.13858929293552  nmf : nmf_nndsvdar\n",
      "106 \t \t 42.13858585218299  nmf : nmf_nndsvda\n",
      "128 \t \t 42.1386080858152  nmf : nmf_nndsvd\n"
     ]
    }
   ],
   "source": [
    "dictOfNmf = {nmf_random:\"nmf_random\", nmf_nndsvdar:\"nmf_nndsvdar\", nmf_nndsvda:\"nmf_nndsvda\", nmf_nndsvd:\"nmf_nndsvd\"}\n",
    "printErrorAndIter(dictOfNmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the differents options of the init from the sklearn library, I obtain that only random gives a  very different results from the others ones that have very close results for all the topics. \n",
    "\n",
    "#### I notice that the error is higher for the random init and all the others methods have the same error (with 10^-4 in accuracy).  Besides, we notice that the nmf_nndsvdar is faster in term of number of iteration than the nmf_nndsvd, as explained in the documentation. \n",
    "\n",
    "#### In addition, we get results that vary a lot between the random and the others, especially in the the Topics {1,3,4,9}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare and comment on the difference between the results obtained with l2 cost com- pared to the generalised Kullback-Liebler cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/decomposition/nmf.py:212: UserWarning: The multiplicative update ('mu') solver cannot update zeros present in the initialization, and so leads to poorer results when used jointly with init='nndsvd'. You may try init='nndsvda' or init='nndsvdar' instead.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "nmf_random_kull,random_messages_kull = build_NMF( beta_loss=\"kullback-leibler\",solver='mu',Verbose=Verbose)\n",
    "nmf_nndsvd_kull,nndsvd_messages_kull = build_NMF(init =\"nndsvd\",beta_loss=\"kullback-leibler\",solver='mu', Verbose=Verbose)\n",
    "nmf_nndsvda_kull,nndsvda_messages_kull = build_NMF(init =\"nndsvda\",beta_loss=\"kullback-leibler\",solver='mu', Verbose=Verbose)\n",
    "nmf_nndsvdar_kull,nndsvdar_messages_kull = build_NMF(init =\"nndsvdar\",beta_loss=\"kullback-leibler\",solver='mu', Verbose=Verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations \t error\t\t name\n",
      "170 \t \t 212.11368422840948  nmf : nmf_random_kull\n",
      "130 \t \t 211.17330591792012  nmf : nmf_nndsvdar_kull\n",
      "100 \t \t 211.11744121234366  nmf : nmf_nndsvda_kull\n",
      "60 \t \t 214.08809055091118  nmf : nmf_nndsvd_kull\n"
     ]
    }
   ],
   "source": [
    "dictOfNmf_kull = {nmf_random_kull :\"nmf_random_kull\", nmf_nndsvdar_kull:\"nmf_nndsvdar_kull\", nmf_nndsvda_kull:\"nmf_nndsvda_kull\", nmf_nndsvd_kull:\"nmf_nndsvd_kull\"}\n",
    "printErrorAndIter(dictOfNmf_kull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/decomposition/nmf.py:156: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(2 * res)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/decomposition/nmf.py:1035: ConvergenceWarning: Maximum number of iteration 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/decomposition/nmf.py:212: UserWarning: The multiplicative update ('mu') solver cannot update zeros present in the initialization, and so leads to poorer results when used jointly with init='nndsvd'. You may try init='nndsvda' or init='nndsvdar' instead.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "nmf_random_it, random_messages_it = build_NMF(beta_loss=\"itakura-saito\",solver='mu',Verbose=Verbose)\n",
    "nmf_nndsvd_it, nndsvd_messages_it = build_NMF(init =\"nndsvd\",beta_loss=\"itakura-saito\",solver='mu', Verbose=Verbose)\n",
    "nmf_nndsvda_it, nndsvda_messages_it = build_NMF(init =\"nndsvda\",beta_loss=\"itakura-saito\",solver='mu', Verbose=Verbose)\n",
    "nmf_nndsvdar_it, nndsvdar_messages_it = build_NMF(init =\"nndsvdar\",beta_loss=\"itakura-saito\",solver='mu', Verbose=Verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations \t error\t\t name\n",
      "200 \t \t nan  nmf : nmf_random_it\n",
      "200 \t \t nan  nmf : nmf_nndsvdar_it\n",
      "200 \t \t nan  nmf : nmf_nndsvda_it\n",
      "200 \t \t nan  nmf : nmf_nndsvd_it\n"
     ]
    }
   ],
   "source": [
    "dictOfNmf_it = {nmf_random_it :\"nmf_random_it\", nmf_nndsvdar_it:\"nmf_nndsvdar_it\", nmf_nndsvda_it:\"nmf_nndsvda_it\", nmf_nndsvd_it:\"nmf_nndsvd_it\"}\n",
    "printErrorAndIter(dictOfNmf_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first results, obtained in the first question are computed with the l2 cost. As observed here, we have around 5 times more error with every methods using Kullback-Liebler  than using the l2 cost.  (42 vs 211).\n",
    "\n",
    "#### In addition, all algorithms take fewer steps with the l2 norm execpt for the nndsvd. for the nndsvd, the number of operation has been divided by two. \n",
    "\n",
    "#### Observing the themes found by these methods, the themes are similar but the Kullback-Liebler seems less accurate.\n",
    "\n",
    "#### Finally, When using the itakura-saito method, I observe that the algorithm reach its max_iter limit at 200, as seen in the course, we cannot predict if this method converge or not, it doesn't here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test and comment on the results obtained using a simpler term-frequency representation as input (as opposed to the TF-IDF representation considered in the code above) when considering the Kullback-Liebler cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_random_kull, random_messages_kull = build_NMF( beta_loss=\"kullback-leibler\",solver='mu',Verbose=Verbose)\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,max_features=n_features,stop_words='english')\n",
    "vectorizer = CountVectorizer(max_df = 0.95, min_df = 2, max_features = n_features, stop_words = 'english')    \n",
    "features = vectorizer.fit_transform(data_samples)\n",
    "nmf_random_kull_count, random_messages_kull_count = build_NMF(tfidf=features, beta_loss=\"kullback-leibler\",solver='mu',Verbose=Verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations \t error\t\t name\n",
      "170 \t \t 212.11368422840948  nmf : kull with tdidf\n",
      "190 \t \t 592.5776601956052  nmf : kull with countVectorizer\n"
     ]
    }
   ],
   "source": [
    "dictOfNmf_kull_test = {nmf_random_kull:\"kull with tdidf\",nmf_random_kull_count:\"kull with countVectorizer\"}\n",
    "printErrorAndIter(dictOfNmf_kull_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The simplier term frequency representation as countvectorizer, with the Kullback-Liebler cost, get a much highier error, with more iterations: around 3 times for the error and 20 iterations more (212 vs 592 and 170 vs 190)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST code for the first part\n",
    "if DEBUG : \n",
    "    for i in range(len(random_messages_it)):\n",
    "        print_messages(i)\n",
    "        print()\n",
    "        print_messages_it(i)\n",
    "        print()\n",
    "    for i in range(len(nndsvdar_messages_kull)):\n",
    "        print(\"TOPIC %d\" %i)\n",
    "        print_messages_kull(i)\n",
    "        print()\n",
    "        print()\n",
    "        print_messages(i)\n",
    "        print()\n",
    "        print()\n",
    "    for i in range(len(nndsvdar_messages)):\n",
    "        print(\"TOPIC %d\" %i)\n",
    "        print_messages(i)\n",
    "        print()\n",
    "        print()\n",
    "    print(check_diff(random_messages_kull,random_messages,\"random_messages kull\"),\n",
    "        check_diff(nndsvd_messages_kull,nndsvd_messages,\"nndsvd_messages kull \"),\n",
    "        check_diff(nndsvda_messages_kull,nndsvda_messages,\"nndsvda_messages kull\"),\n",
    "        check_diff(nndsvdar_messages_kull,nndsvdar_messages,\"nndsvdar_messages kull\"))\n",
    "    \n",
    "    print(check_diff(random_messages_it, random_messages, \"random_messages it\"),\n",
    "    check_diff(nndsvd_messages_it, nndsvd_messages, \"nndsvd_messages it \"),\n",
    "    check_diff(nndsvda_messages_it, nndsvda_messages, \"nndsvda_messages it\"),\n",
    "    check_diff(nndsvdar_messages_it, nndsvdar_messages, \"nndsvdar_messages it\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - CUSTOM NMF IMPLEMENTATION -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Implement the multiplicative update rules (derived from the majorisation-minimisation approach) for NMF estimation with β divergences, including the case β = 1 (generalised Kullback-Liebler divergence). Ensure that :\n",
    "                                        \n",
    " \n",
    "1. you can easily choose a custom initialisation for the W and H matrices ;\n",
    "2. you can set a custom number of iteration ;\n",
    "3. you can monitor the behaviour of the loss function across the iterations and that it is readily decreasing.\n",
    "Compare your implementation with the one offered by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_nmf :\n",
    "    \n",
    "        \n",
    "    def __init__(self, features, beta,W,H, k=2,tole=0.01):\n",
    "        self.k = k\n",
    "        self.tole = tole\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.features = features\n",
    "        \n",
    "        self.W = np.random.rand(features.shape[0],k)\n",
    "        self.H = np.random.rand(k,features.shape[1])\n",
    "        \n",
    "        if(beta == 0 ):\n",
    "            self.betafunction = self.itakura_saito\n",
    "        elif(beta == 1):\n",
    "            self.betafunction = self.kullback_leiber\n",
    "        else :\n",
    "            self.betafunction = self.euclidean_distance\n",
    "\n",
    "    \n",
    "    def euclidean_distance(self, x, y, beta):\n",
    "        return (1 / ( beta*( beta - 1) ) )*(np.pow(x, beta) + (beta - 1)*np.pow(y, beta) - beta*x*np.pow(y, beta - 1))\n",
    "    \n",
    "    def kullback_leiber(self, x, y, beta):\n",
    "        return x * np.log(x / y) - x + y\n",
    "    \n",
    "    def itakura_saito(self, x, y, beta):\n",
    "        return (x / y) - np.log( x / y) - 1\n",
    "    \n",
    "    def get_error(self):\n",
    "        W,H,features = self.W,self.H,self.features\n",
    "        function =  self.betafunction\n",
    "        WH = np.dot(W, H)\n",
    "        err = 0 \n",
    "        for i in range(features.shape[0]):\n",
    "            for j in range(features.shape[1]):\n",
    "                x = features[:,j][i][0]\n",
    "                x = np.squeeze(np.asarray(x))\n",
    "                y = WH[i][j]\n",
    "                if (x == 0 or np.isnan(x)) :\n",
    "                    break\n",
    "                if(y == 0 or np.isnan(y)):\n",
    "                    break\n",
    "                #x = sys.float_info.epsilon\n",
    "                #y = np.squeeze(np.asarray(WH[i][j]))\n",
    "                #y = WH[:,j][i][0]\n",
    "                #print(\"y:\",y,\"WH:\", WH)\n",
    "                #if(y == 0 ):\n",
    "                #    break\n",
    "                res = function(x,y,beta)\n",
    "                if( not np.isnan(res)):\n",
    "                    err += function(x,y,beta)\n",
    "        return err\n",
    "    \n",
    "    \n",
    "    def nmf(self,max_iter=200):\n",
    "        W,H,features,beta = self.W,self.H,self.features,self.beta\n",
    "        init_err = self.get_error()\n",
    "        err = init_err\n",
    "        for it in range(max_iter):\n",
    "            W,H = self.W, self.H\n",
    "            WH = np.dot(W,H)\n",
    "            WH_BETA  = np.power(WH,beta-2)\n",
    "            num = np.dot(W.T, np.multiply(WH_BETA, features))\n",
    "            dem = np.dot(W.T, np.power(WH,beta-1))\n",
    "            term = np.divide(num, dem)\n",
    "            H = np.multiply(H, term)\n",
    "            \n",
    "            self.H = np.squeeze(np.asarray(H))\n",
    "            print(self.H)\n",
    "            \n",
    "            W,H = self.W, self.H\n",
    "            WH = np.dot(W,H)\n",
    "            WH_BETA  = np.power(WH,beta - 2)\n",
    "            num = np.dot(np.multiply(WH_BETA, features),H.T)\n",
    "            dem = np.dot(np.power(WH,beta - 1), H.T)\n",
    "            term = np.divide(num, dem)\n",
    "            W = np.multiply(W, term)\n",
    "            self.W = np.squeeze(np.asarray(W))\n",
    "            \n",
    "\n",
    "\n",
    "            current_err = self.get_error()\n",
    "            progres = ((err - current_err) / init_err)\n",
    "            print(\"err: \",current_err,\"it:\",it, \"err_init: \", init_err)\n",
    "            print(\"progres: \",progres,\"it:\",it)\n",
    "            \n",
    "            err = current_err\n",
    "            if(progres < self.tole and progres > 0):\n",
    "                break\n",
    "            \n",
    "        return self.W, self.H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.87037313844045\n"
     ]
    }
   ],
   "source": [
    "features = tfidf.todense()\n",
    "\n",
    "beta = 1 \n",
    "W, H = None, None\n",
    "customNMF = Custom_nmf(features, beta, W, H)\n",
    "\n",
    "print(customNMF.get_error())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00702013 0.0050419  0.00831683 ... 0.00630791 0.00952164 0.00273914]\n",
      " [0.00595883 0.00350593 0.00866043 ... 0.01338394 0.00557994 0.00170405]]\n",
      "err:  44.89977853682493 it: 0 err_init:  16.87037313844045\n",
      "progres:  -1.6614573470528233 it: 0\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "err:  0 it: 1 err_init:  16.87037313844045\n",
      "progres:  2.6614573470528233 it: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:64: RuntimeWarning: divide by zero encountered in power\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:65: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "err:  0 it: 2 err_init:  16.87037313844045\n",
      "progres:  0.0 it: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        ...,\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan]]), array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customNMF.nmf(max_iter =3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I created a programm that conforms with the first two obligations (set up W, H and set a custom number of iteration). It's easely setable with this implementation with object programming.\n",
    "#### But  I still have problems with the actualisation of the W,H matrix, the convertion of csr_matrix to ndarray seems to convert W,H to nadrray of nan that make the algortithm useless after 2 iterations. However, in this case, after two iterations the error decrease. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
